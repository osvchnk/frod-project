services:

  db:
    image: postgres:latest
    restart: always
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: postgres
    ports:
      - "5435:5432"
    command:
      - "postgres"
      - "-c"
      - "wal_level=logical"
      - "-c"
      - "max_replication_slots=10"
      - "-c"
      - "max_wal_senders=10"
    volumes:
      - pg_data:/var/lib/postgresql/data
    networks:
      - app-net

  adminer:
    image: adminer
    restart: always
    ports:
      - 8081:8080
    networks:
      - app-net

  zookeeper:
    image: confluentinc/cp-zookeeper:7.2.15
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
    networks:
      - app-net

  kafka:
    image: confluentinc/cp-kafka:7.2.15
    depends_on:
      - zookeeper
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_LISTENERS: INTERNAL://:29092,EXTERNAL://:9092
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka:29092,EXTERNAL://localhost:9092
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
    networks:
      - app-net

  kafdrop:
    image: obsidiandynamics/kafdrop:4.1.0
    container_name: kafdrop
    ports:
      - "9090:9000"
    environment:
      KAFKA_BROKERCONNECT: kafka:29092  # Connects to Kafka's internal listener
    depends_on:
      - kafka
    networks:
      - app-net

  connect:
    image: debezium/connect:3.0.0.Final
    ports:
      - 8083:8083
      - 5005:5005
      - 8778:8778
    environment:
      - BOOTSTRAP_SERVERS=kafka:29092
      - GROUP_ID=1
      - CONFIG_STORAGE_TOPIC=my_connect_configs
      - OFFSET_STORAGE_TOPIC=my_connect_offsets
      - STATUS_STORAGE_TOPIC=my_connect_statuses
    networks:
      - app-net


  debezium-ui:
    image: debezium/debezium-ui:latest
    ports:
      - 8090:8080
    environment:
      - KAFKA_CONNECT_URIS=http://connect:8083
    depends_on:
      - connect
    networks:
      - app-net

  spark-master:
    image: bitnami/spark:3.5.0
    container_name: spark-master
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_HADOOP_FS_DEFAULTFS=hdfs://namenode:9000
    ports:
      - "8080:8080"  # Spark master web UI
      - "7077:7077"  # Spark master port
    networks:
      - app-net

  spark-worker-1:
    image: bitnami/spark:3.5.0
    container_name: spark-worker-1
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_HADOOP_FS_DEFAULTFS=hdfs://namenode:9000
    depends_on:
      - spark-master
    ports:
      - "8091:8081"  # Spark worker web UI
    networks:
      - app-net

  spark-worker-2:
    image: bitnami/spark:3.5.0
    container_name: spark-worker-2
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_HADOOP_FS_DEFAULTFS=hdfs://namenode:9000
    depends_on:
      - spark-master
    ports:
      - "8092:8081"
    networks:
      - app-net

  spark-worker-3:
    image: bitnami/spark:3.5.0
    container_name: spark-worker-3
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_HADOOP_FS_DEFAULTFS=hdfs://namenode:9000
    depends_on:
      - spark-master
    ports:
      - "8093:8081"
    networks:
      - app-net

  spark-worker-4:
    image: bitnami/spark:3.5.0
    container_name: spark-worker-4
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_HADOOP_FS_DEFAULTFS=hdfs://namenode:9000
    depends_on:
      - spark-master
    ports:
      - "8094:8081"
    networks:
      - app-net

  pyspark-app:
    build: ./pyspark-app
    volumes:
      - ./pyspark-app:/app
    depends_on:
      - spark-master
    networks:
      - app-net

  namenode:
    image: apache/hadoop:3
    hostname: namenode
    container_name: namenode
    command: [ "hdfs", "namenode" ]
    ports:
      - 9870:9870     # NameNode web UI
      - 9000:9000     # HDFS RPC
    environment:
      - ENSURE_NAMENODE_DIR=/tmp/hadoop-root/dfs/name
    volumes:
      - ./configs/core-site.xml:/opt/hadoop/etc/hadoop/core-site.xml
      - ./configs/hdfs-site.xml:/opt/hadoop/etc/hadoop/hdfs-site.xml
      - ./configs/entrypoint.sh:/entrypoint.sh
      - namenode_data:/tmp/hadoop-root/dfs/name
    entrypoint: ["/bin/bash", "/entrypoint.sh"]
    networks:
      - app-net

  datanode1:
    image: apache/hadoop:3
    hostname: datanode1
    container_name: datanode1
    command: [ "hdfs", "datanode" ]
    volumes:
      - ./configs/core-site.xml:/opt/hadoop/etc/hadoop/core-site.xml
      - ./configs/hdfs-site.xml:/opt/hadoop/etc/hadoop/hdfs-site.xml
      - datanode1_data:/tmp/hadoop-root/dfs/data
    networks:
      - app-net
    depends_on:
      - namenode

  datanode2:
    image: apache/hadoop:3
    hostname: datanode2
    container_name: datanode2
    command: [ "hdfs", "datanode" ]
    volumes:
      - ./configs/core-site.xml:/opt/hadoop/etc/hadoop/core-site.xml
      - ./configs/hdfs-site.xml:/opt/hadoop/etc/hadoop/hdfs-site.xml
      - datanode2_data:/tmp/hadoop-root/dfs/data
    networks:
      - app-net
    depends_on:
      - namenode

volumes:
  pg_data:
  namenode_data:
  datanode1_data:
  datanode2_data:

networks:
  app-net:
